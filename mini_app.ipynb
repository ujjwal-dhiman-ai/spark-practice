{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdfc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PySpark shell or Jupyter notebook\n",
    "from mini_app import SCD2MiniApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d716336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 days of sample data...\n",
      "  Generated snapshot 20250801 with 100 records\n",
      "  Generated snapshot 20250802 with 100 records\n",
      "  Generated snapshot 20250803 with 100 records\n",
      "  Generated snapshot 20250804 with 100 records\n",
      "  Generated snapshot 20250805 with 100 records\n",
      "  Generated snapshot 20250806 with 100 records\n",
      "  Generated snapshot 20250807 with 100 records\n",
      "  Generated snapshot 20250808 with 98 records\n",
      "  Generated snapshot 20250809 with 98 records\n",
      "  Generated snapshot 20250810 with 98 records\n"
     ]
    }
   ],
   "source": [
    "app = SCD2MiniApp()\n",
    "app.generate_sample_data(num_days=10, records_per_day=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e8fec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing snapshots from 20250801 to 20250810\n",
      "Checkpoint frequency: every 2 snapshots\n",
      "\n",
      "================================================================================\n",
      "Processing snapshot: 20250801\n",
      "  Input records: 100\n",
      "    Processing SCD2 changes for 20250801\n",
      "    No historical data, returning current snapshot as full history\n",
      "  Total history records: 100\n",
      "  Active records: 100\n",
      "\n",
      "================================================================================\n",
      "Processing snapshot: 20250802\n",
      "  Input records: 100\n",
      "    Processing SCD2 changes for 20250802\n",
      "    Current records: 100\n",
      "    History active records: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-08-15 12:39:50.271\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[AMBIGUOUS_REFERENCE] Reference `FirstSnapshot` is ambiguous, could be: [`FirstSnapshot`, `FirstSnapshot`]. SQLSTATE: 42704\", \"context\": {\"file\": \"line 221 in cell [10]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"AMBIGUOUS_REFERENCE\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o647.withColumn.\\n: org.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `FirstSnapshot` is ambiguous, could be: [`FirstSnapshot`, `FirstSnapshot`]. SQLSTATE: 42704\\r\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)\\r\\n\\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)\\r\\n\\tat scala.collection.immutable.List.map(List.scala:247)\\r\\n\\tat scala.collection.immutable.List.map(List.scala:79)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)\\r\\n\\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)\\r\\n\\tat scala.collection.immutable.List.map(List.scala:251)\\r\\n\\tat scala.collection.immutable.List.map(List.scala:79)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\\r\\n\\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\\r\\n\\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\\r\\n\\tat scala.collection.immutable.List.foldLeft(List.scala:79)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\\r\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\r\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\r\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\r\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:232)\\r\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\r\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\r\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\r\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\r\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\r\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\r\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\r\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\r\\n\\tat java.base/java.lang.Thread.run(Thread.java:842)\\r\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\r\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(QueryCompilationErrors.scala:2163)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:356)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:164)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$1(ColumnResolutionHelper.scala:520)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(ColumnResolutionHelper.scala:172)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:104)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:179)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)\\r\\n\\t\\tat scala.collection.immutable.List.map(List.scala:247)\\r\\n\\t\\tat scala.collection.immutable.List.map(List.scala:79)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:708)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$9(ColumnResolutionHelper.scala:202)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(ColumnResolutionHelper.scala:202)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(ColumnResolutionHelper.scala:145)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(ColumnResolutionHelper.scala:209)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(ColumnResolutionHelper.scala:527)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(ColumnResolutionHelper.scala:513)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.resolveExpressionByPlanChildren(Analyzer.scala:1400)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.$anonfun$applyOrElse$107(Analyzer.scala:1566)\\r\\n\\t\\tat scala.collection.immutable.List.map(List.scala:251)\\r\\n\\t\\tat scala.collection.immutable.List.map(List.scala:79)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1566)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$14.applyOrElse(Analyzer.scala:1435)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1435)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences.apply(Analyzer.scala:1400)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\\r\\n\\t\\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\\r\\n\\t\\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\\r\\n\\t\\tat scala.collection.immutable.List.foldLeft(List.scala:79)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\\r\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\r\\n\\t\\t... 23 more\\r\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"c:\\\\codes\\\\practice\\\\spark-practice\\\\.venv\\\\Lib\\\\site-packages\\\\pyspark\\\\errors\\\\exceptions\\\\captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"c:\\\\codes\\\\practice\\\\spark-practice\\\\.venv\\\\Lib\\\\site-packages\\\\py4j\\\\protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ERROR: Failed processing 20250802: [AMBIGUOUS_REFERENCE] Reference `FirstSnapshot` is ambiguous, could be: [`FirstSnapshot`, `FirstSnapshot`]. SQLSTATE: 42704\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `FirstSnapshot` is ambiguous, could be: [`FirstSnapshot`, `FirstSnapshot`]. SQLSTATE: 42704",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_snapshots\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m20250801\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m20250810\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\codes\\practice\\spark-practice\\mini_app.py:310\u001b[39m, in \u001b[36mSCD2MiniApp.process_snapshots\u001b[39m\u001b[34m(self, start_date, end_date, checkpoint_every)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Input records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_df.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Process SCD2 changes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m new_scd2_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_scd2_changes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscd2_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_order\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# CRITICAL: Proper DataFrame state management\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scd2_df.is_cached:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\codes\\practice\\spark-practice\\mini_app.py:221\u001b[39m, in \u001b[36mSCD2MiniApp.process_scd2_changes\u001b[39m\u001b[34m(self, current_df, history_df, snapshot_date, column_order)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Process new and updated records (insert new versions)\u001b[39;00m\n\u001b[32m    216\u001b[39m new_and_updated_keys = joined_df.filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mHistoryStatus\u001b[39m\u001b[33m\"\u001b[39m).isin([\u001b[33m\"\u001b[39m\u001b[33mnew\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mupdated\u001b[39m\u001b[33m\"\u001b[39m])).select(\u001b[33m\"\u001b[39m\u001b[33mBusinessKey\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m new_records = (\n\u001b[32m    218\u001b[39m     \u001b[43mcurrent_df\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_and_updated_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBusinessKey\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msemi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_lookup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBusinessKey\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFirstSnapshot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFirstSnapshot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msnapshot_date\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     .select(*column_order)\n\u001b[32m    223\u001b[39m )\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Process unchanged records\u001b[39;00m\n\u001b[32m    226\u001b[39m unchanged_records = (\n\u001b[32m    227\u001b[39m     history_df\n\u001b[32m    228\u001b[39m     .join(deleted_keys, [\u001b[33m\"\u001b[39m\u001b[33mBusinessKey\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mleft_anti\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m     .join(updated_keys, [\u001b[33m\"\u001b[39m\u001b[33mBusinessKey\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mleft_anti\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    230\u001b[39m     .select(*column_order)\n\u001b[32m    231\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\codes\\practice\\spark-practice\\.venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1623\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\codes\\practice\\spark-practice\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\codes\\practice\\spark-practice\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [AMBIGUOUS_REFERENCE] Reference `FirstSnapshot` is ambiguous, could be: [`FirstSnapshot`, `FirstSnapshot`]. SQLSTATE: 42704"
     ]
    }
   ],
   "source": [
    "app.process_snapshots(\"20250801\", \"20250810\", checkpoint_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.analyze_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6c2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d62f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ujjwa\\\\AppData\\\\Local\\\\Temp'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempfile.gettempdir()  # This will return the path to the temporary directory used by the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65afea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SCD2 Demo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f973b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(r\"C:\\Users\\ujjwa\\AppData\\Local\\Temp\\scd2_demo\\scd2_history.parquet\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc25661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
